{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import parquet data into pandas dataframe\n",
    "import pandas as pd\n",
    "df_train = pd.read_parquet('relevance_train.parquet')\n",
    "df_test = pd.read_parquet('relevance_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIGHT NEED TO UNCOMMENT SOME COMMANDS WHEN RUNNING CODE FOR FIRST TIME\n",
    "\n",
    "# Import library for tokenisation\n",
    "import nltk \n",
    "#Download punctuation infrormation\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#Import regular expressions library\n",
    "import re\n",
    "#Download stopwords information\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#Import stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "#Import lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch stopwords in English\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stop words from a sentence \n",
    "def remove_stop_words(sentence): \n",
    "  # Split the sentence into individual words \n",
    "  words = sentence.split() \n",
    "  \n",
    "  # List comprehension to remove stop words \n",
    "  filtered_words = [word for word in words if word not in stopset] \n",
    "  \n",
    "  # Join the filtered words back into a sentence \n",
    "  return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to stem all words in a sentence\n",
    "ps = PorterStemmer()\n",
    "def stem (sentence):\n",
    "    stemmed = []\n",
    "    # Split the sentence into individual words \n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        stemmed.append(ps.stem(word))\n",
    "    #Join the stemmed words back into a sentence\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to find all lemmas of the words in a sentence\n",
    "wl = WordNetLemmatizer()\n",
    "def lemma (sentence):\n",
    "    lemmas = []\n",
    "    # Split the sentence into individual words \n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        lemmas.append(wl.lemmatize(word))\n",
    "    #Join the stemmed words back into a sentence\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to perform language processing on dataframe collumn\n",
    "def nlp (iterable):\n",
    "    processed = []    \n",
    "    for element in iterable:\n",
    "        element = str(element)\n",
    "        element = element.lower()\n",
    "        element = ''.join([i for i in element if not i.isdigit()]) #Remove digits from string\n",
    "        element = re.sub(\"\\<.*?\\>\",\" \", element) #Remove all HTML tags\n",
    "        element = element.replace('\\\\n', '') #Remove new line token\n",
    "        element = re.sub(r'[^\\w\\s]','',element) #Remove punctuation\n",
    "        element = remove_stop_words(element) #Call remove stopwords function\n",
    "        #element = lemma(element) #does not add much to the stemming\n",
    "        element = stem(element)\n",
    "        #Add processed element into the list\n",
    "        processed.append(element)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For columns with classifiable text, create classification (label encoding)\n",
    "#Note the numeric value is arbitrary so no numeric analysis is to be performed\n",
    "def id_creator (iterable):\n",
    "    id = []\n",
    "    id_Dict = {}\n",
    "    for item in iterable:\n",
    "        item = str(item)\n",
    "        if item not in id_Dict:\n",
    "            id_Dict[item] = len(id_Dict) + 1\n",
    "        id.append(id_Dict[item])\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate DataFrame with the processed text for train\n",
    "processed_train = {'id': df_train['doc_id'], \n",
    "                   'author': df_train['author'], \n",
    "                   'title': nlp(df_train['title']),\n",
    "                   'topic_id': df_train['topic_id'],\n",
    "                   'body': nlp(df_train['body']),\n",
    "                   'description_id': id_creator(df_train['description']),\n",
    "                   'narrative_id': id_creator(df_train['narrative']),\n",
    "                   'judgement':df_train['judgement']}\n",
    "df_train_processed = pd.DataFrame(processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate DataFrame with the processed text for test \n",
    "descriptions = list(id_creator(df_train['description'].append(df_test['description'])))\n",
    "description_test = descriptions[len(df_train['description']):]\n",
    "narrative = list(id_creator(df_train['narrative'].append(df_test['narrative'])))\n",
    "narrative_test = descriptions[len(df_train['narrative']):]\n",
    "\n",
    "processed_test = {'id': df_test['doc_id'], \n",
    "                   'author': df_test['author'], \n",
    "                   'title': nlp(df_test['title']),\n",
    "                   'topic_id': df_test['topic_id'],\n",
    "                   'body': nlp(df_test['body']),\n",
    "                   'description_id': description_test,\n",
    "                   'narrative_id': narrative_test}\n",
    "df_test_processed = pd.DataFrame(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export processed text to excel \n",
    "df_train_processed.to_excel('relevance_train_processed.xlsx')\n",
    "df_test_processed.to_excel('relevance_test_processed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
