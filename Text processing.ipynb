{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2962935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                doc_id  judgement  \\\n",
      "0               0      004c6120d0aa69da29cc045da0562168          0   \n",
      "1               1      005a1f0c2064487a7f15443b2a5f349a          0   \n",
      "2               2  00722094-2935-11e2-b4e0-346287b7e56c          0   \n",
      "3               3  007d2856-7cc4-11e4-84d4-7c896b90abdc          0   \n",
      "4               4  009aafb6-0283-11e6-8bb1-f124a43f84dc          0   \n",
      "...           ...                                   ...        ...   \n",
      "21115       26228  fd63b1f8-f00e-11e4-a55f-38924fca94f9          0   \n",
      "21116       26229  fdeefde0-44e1-11e4-b47c-f5889e061e5f          0   \n",
      "21117       26230      fe320ce7929e640c70458009f73e5241          0   \n",
      "21118       26231      ff1c523ea149f03d89019adb8782cdd9          0   \n",
      "21119       26232  ff3a25b0-0ba4-11e4-8341-b8072b1e7348          0   \n",
      "\n",
      "                  author                                               body  \\\n",
      "0         Terrence McCoy  <p>More than 60 years ago, a fair-skinned Iris...   \n",
      "1          Brian McNoldy  <p>Hurricane Fred, which formed over the weeke...   \n",
      "2        Editorial Board  <p>EIGHT YEARS AGO this month, an inspiring mo...   \n",
      "3         Adam Bernstein  <p>Mary Burke Washington, an economist who was...   \n",
      "4                    NaN  <p>When Treasury Secretary Jack Lew <a href=\"h...   \n",
      "...                  ...                                                ...   \n",
      "21115  Ashley Halsey III  <p>Almost two years after a runaway tank-car t...   \n",
      "21116     George F. Will  <p><span class=\"dateline\">URBANDALE, Iowa</spa...   \n",
      "21117       Daron Taylor  <p>There’s a secret history of bacon that almo...   \n",
      "21118      Becky Krystal  <p>You may have noticed that we’re focusing on...   \n",
      "21119                NaN                                                NaN   \n",
      "\n",
      "                     byline  \\\n",
      "0         By Terrence McCoy   \n",
      "1          By Brian McNoldy   \n",
      "2        By Editorial Board   \n",
      "3         By Adam Bernstein   \n",
      "4                       NaN   \n",
      "...                     ...   \n",
      "21115  By Ashley Halsey III   \n",
      "21116     By George F. Will   \n",
      "21117       By Daron Taylor   \n",
      "21118      By Becky Krystal   \n",
      "21119                   NaN   \n",
      "\n",
      "                                                   title  topic_id  \\\n",
      "0      Report on Irish baby homes documents use of in...       321   \n",
      "1      Hurricane Fred is a fountain of ‘firsts’ in th...       321   \n",
      "2                     Ukraine slides away from democracy       321   \n",
      "3      Mary Washington, government official and widow...       321   \n",
      "4                      Will women be shortchanged again?       321   \n",
      "...                                                  ...       ...   \n",
      "21115  Transportation Dept. gets strict on rules for ...       825   \n",
      "21116  Joni Ernst’s Iowa campaign makes quick work of...       825   \n",
      "21117  The real reason why Americans love bacon for b...       825   \n",
      "21118  You’re all ears, and so are we: Our favorite c...       825   \n",
      "21119                                                NaN       825   \n",
      "\n",
      "                                             description  \\\n",
      "0      Pertinent documents will reflect the fact that...   \n",
      "1      Pertinent documents will reflect the fact that...   \n",
      "2      Pertinent documents will reflect the fact that...   \n",
      "3      Pertinent documents will reflect the fact that...   \n",
      "4      Pertinent documents will reflect the fact that...   \n",
      "...                                                  ...   \n",
      "21115  Does diversion of U.S. corn crops into ethanol...   \n",
      "21116  Does diversion of U.S. corn crops into ethanol...   \n",
      "21117  Does diversion of U.S. corn crops into ethanol...   \n",
      "21118  Does diversion of U.S. corn crops into ethanol...   \n",
      "21119  Does diversion of U.S. corn crops into ethanol...   \n",
      "\n",
      "                                               narrative  \\\n",
      "0      Pertinent documents relating to this issue wil...   \n",
      "1      Pertinent documents relating to this issue wil...   \n",
      "2      Pertinent documents relating to this issue wil...   \n",
      "3      Pertinent documents relating to this issue wil...   \n",
      "4      Pertinent documents relating to this issue wil...   \n",
      "...                                                  ...   \n",
      "21115  Identify documents that discuss the impact of ...   \n",
      "21116  Identify documents that discuss the impact of ...   \n",
      "21117  Identify documents that discuss the impact of ...   \n",
      "21118  Identify documents that discuss the impact of ...   \n",
      "21119  Identify documents that discuss the impact of ...   \n",
      "\n",
      "                   topic_title  \n",
      "0         Women in Parliaments  \n",
      "1         Women in Parliaments  \n",
      "2         Women in Parliaments  \n",
      "3         Women in Parliaments  \n",
      "4         Women in Parliaments  \n",
      "...                        ...  \n",
      "21115  ethanol and food prices  \n",
      "21116  ethanol and food prices  \n",
      "21117  ethanol and food prices  \n",
      "21118  ethanol and food prices  \n",
      "21119  ethanol and food prices  \n",
      "\n",
      "[21120 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import data into pandas dataframe\n",
    "import pandas as pd\n",
    "df_train = pd.read_excel('relevance_train.xlsx')\n",
    "df_test = pd.read_excel('relevance_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4333ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIGHT NEED TO UNCOMMENT SOME COMMANDS WHEN RUNNING CODE FOR FIRST TIME\n",
    "\n",
    "# Import library for tokenisation\n",
    "import nltk \n",
    "#Download punctuation infrormation\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#Import regular expressions library\n",
    "import re\n",
    "#Download stopwords information\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#Import stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "#Import lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "\n",
    "#Fetch stopwords in English\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stop words from a sentence \n",
    "def remove_stop_words(sentence): \n",
    "  # Split the sentence into individual words \n",
    "  words = sentence.split() \n",
    "  \n",
    "  # List comprehension to remove stop words \n",
    "  filtered_words = [word for word in words if word not in stopset] \n",
    "  \n",
    "  # Join the filtered words back into a sentence \n",
    "  return ' '.join(filtered_words)\n",
    "\n",
    "#Define a function to stem all words in a sentence\n",
    "ps = PorterStemmer()\n",
    "def stem (sentence):\n",
    "    stemmed = []\n",
    "    # Split the sentence into individual words \n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        stemmed.append(ps.stem(word))\n",
    "    #Join the stemmed words back into a sentence\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "#Define a function to find all lemmas of the words in a sentence\n",
    "wl = WordNetLemmatizer()\n",
    "def lemma (sentence):\n",
    "    lemmas = []\n",
    "    # Split the sentence into individual words \n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        lemmas.append(wl.lemmatize(word))\n",
    "    #Join the stemmed words back into a sentence\n",
    "    return ' '.join(lemmas)\n",
    "        \n",
    "#Define a function to perform language processing on dataframe collumn\n",
    "def nlp (iterable):\n",
    "    processed = []    \n",
    "    for element in iterable:\n",
    "        element = str(element)\n",
    "        element = element.lower()\n",
    "        element = ''.join([i for i in element if not i.isdigit()]) #Remove digits from string\n",
    "        element = re.sub(\"\\<.*?\\>\",\" \", element) #Remove all HTML tags\n",
    "        element = element.replace('\\\\n', '') #Remove new line token\n",
    "        element = re.sub(r'[^\\w\\s]','',element) #Remove punctuation\n",
    "        element = remove_stop_words(element) #Call remove stopwords function\n",
    "        #element = lemma(element) #does not add much to the stemming\n",
    "        element = stem(element)\n",
    "        #Add processed element into the list\n",
    "        processed.append(element)\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "32f5f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For columns with classifiable text, create classification\n",
    "#Note the numeric value is arbitrary so no numeric analysis is to be performed\n",
    "def id_creator (iterable):\n",
    "    id = []\n",
    "    id_Dict = {}\n",
    "    for item in iterable:\n",
    "        item = str(item)\n",
    "        if item not in id_Dict:\n",
    "            id_Dict[item] = len(id_Dict) + 1\n",
    "        id.append(id_Dict[item])\n",
    "    return id\n",
    "\n",
    "\n",
    "#Generate DataFrame with the processed text\n",
    "processed_train = {'id': df_train['Unnamed: 0'], \n",
    "                   'author': df_train['author'], \n",
    "                   'title': nlp(df_train['title']),\n",
    "                   'topic_id': df_train['topic_id'],\n",
    "                   'body': nlp(df_train['body']),\n",
    "                   'description_id': id_creator(df_train['description']),\n",
    "                   'narrative_id': id_creator(df_train['narrative']),\n",
    "                   'judgement':df_train['judgement']}\n",
    "df_train_processed = pd.DataFrame(processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b5f7b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "         ..\n",
      "21115    40\n",
      "21116    40\n",
      "21117    40\n",
      "21118    40\n",
      "21119    40\n",
      "Name: description_id, Length: 21120, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train_processed['description_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "070a86d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njb23192\\AppData\\Local\\Temp\\ipykernel_14652\\2943614820.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  descriptions = list(id_creator(df_train['description'].append(df_test['description'])))\n",
      "C:\\Users\\njb23192\\AppData\\Local\\Temp\\ipykernel_14652\\2943614820.py:4: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  narrative = list(id_creator(df_train['narrative'].append(df_test['narrative'])))\n"
     ]
    }
   ],
   "source": [
    "#Generate DataFrame with the processed text for test \n",
    "descriptions = list(id_creator(df_train['description'].append(df_test['description'])))\n",
    "description_test = descriptions[len(df_train['description']):]\n",
    "narrative = list(id_creator(df_train['narrative'].append(df_test['narrative'])))\n",
    "narrative_test = descriptions[len(df_train['narrative']):]\n",
    "\n",
    "processed_test = {'id': df_test['Unnamed: 0'], \n",
    "                   'author': df_test['author'], \n",
    "                   'title': nlp(df_test['title']),\n",
    "                   'topic_id': df_test['topic_id'],\n",
    "                   'body': nlp(df_test['body']),\n",
    "                   'description_id': description_test,\n",
    "                   'narrative_id': narrative_test}\n",
    "df_test_processed = pd.DataFrame(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dc563578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export processed text to excel \n",
    "df_train_processed.to_excel('relevance_train_processed.xlsx')\n",
    "df_test_processed.to_excel('relevance_test_processed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ae802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
